{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required libraries and preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\anush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "from gensim.models import LdaModel\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Download necessary NLP datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load NLP models\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans text by removing punctuation, tokenizing sentences, and removing stopwords.\"\"\"\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Removing punctuation\n",
    "    sentences = sent_tokenize(text)  # Tokenizing sentences\n",
    "    clean_sentences = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        tokens = [word.lower() for word in sent.split() if word.lower() not in stop_words]\n",
    "        clean_sentences.append(\" \".join(tokens))  # Fix: Missing space in join()\n",
    "\n",
    "    return clean_sentences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task Indentification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identical_task(sentences):\n",
    "    \"\"\"Identifies sentences containing action-related keywords.\"\"\"\n",
    "    task_keywords = {\"must\", \"do\", \"have to\", \"need to\", \"needs to\", \"should\", \"required to\", \n",
    "                     \"scheduled to\", \"is due\", \"due\", \"complete\", \"submit\", \"finish\", \"pay\"}\n",
    "    \n",
    "    identified_tasks = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        doc = nlp(sent)\n",
    "        verbs = {token.lemma_ for token in doc if token.pos_ == \"VERB\"}\n",
    "        \n",
    "        # If sentence has an action keyword or verb, consider it a task\n",
    "        if any(kw in sent for kw in task_keywords) or verbs:\n",
    "            identified_tasks.append(sent)\n",
    "    \n",
    "    return identified_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding assigned person and their deadline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractEntities(task_sentences):\n",
    "    \"\"\"Extracts person responsible and deadline from sentences.\"\"\"\n",
    "    extracted_data = []\n",
    "\n",
    "    for sent in task_sentences:\n",
    "        doc = nlp(sent)\n",
    "        person = None\n",
    "        deadline = None\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                person = ent.text\n",
    "            if ent.label_ in [\"DATE\", \"TIME\"]:\n",
    "                deadline = ent.text\n",
    "\n",
    "        extracted_data.append({\"Task\": sent, \"Assigned to\": person, \"Deadline\": deadline})\n",
    "\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorizing dynamically using topic modeling (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorizeTask(task_sentences):\n",
    "    \"\"\"Uses LDA topic modeling to categorize tasks.\"\"\"\n",
    "    tokenized_tasks = [sent.split() for sent in task_sentences]\n",
    "    dictionary = corpora.Dictionary(tokenized_tasks)\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_tasks]\n",
    "\n",
    "    # Train LDA model\n",
    "    lda_model = LdaModel(corpus, num_topics=3, id2word=dictionary, passes=10)\n",
    "\n",
    "    # Assign topics to tasks\n",
    "    categorized_tasks = []\n",
    "    for i, task in enumerate(task_sentences):\n",
    "        topic = max(lda_model[corpus[i]], key=lambda x: x[1])[0]\n",
    "        categorized_tasks.append({\"Task\": task, \"Category\": f\"Category {topic}\"})\n",
    "\n",
    "    return categorized_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running  the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAndCategorizeTask(text):\n",
    "    \"\"\"Pipeline: preprocess -> extract tasks -> extract entities -> categorize tasks.\"\"\"\n",
    "    preprocessed_sentences = preprocess_text(text)\n",
    "    task_sentences = identical_task(preprocessed_sentences)\n",
    "    extracted_entities = extractEntities(task_sentences)\n",
    "    categorized_tasks = categorizeTask(task_sentences)\n",
    "\n",
    "    # Merge extracted tasks with categories\n",
    "    final_output = []\n",
    "    for i, task in enumerate(extracted_entities):\n",
    "        task[\"Category\"] = categorized_tasks[i][\"Category\"]\n",
    "        final_output.append(task)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Task': 'rahul wakes early every day goes college morning comes back 3 pm present rahul outside buying snacks us john must submit assignment tomorrow evening sarah needs pay rent 1st next month', 'Assigned to': 'sarah', 'Deadline': 'early every day', 'Category': 'Category 1'}]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"\n",
    " \"Rahul wakes early every day, goes to college in the morning, and comes back at 3 pm.\",\n",
    "    \"At present, Rahul is outside buying snacks for us.\",\n",
    "    \"John must submit the assignment by tomorrow evening.\",\n",
    "    \"Sarah needs to pay rent by the 1st of next month.\"\n",
    "\"\"\"\n",
    "\n",
    "# Run Function\n",
    "tasks = extractAndCategorizeTask(input_text)\n",
    "print(tasks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llmVemv)",
   "language": "python",
   "name": "llmvemv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
